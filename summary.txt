########################
# Additional Files
########################
# output.txt
# data
# tokenizer

########################
# Filled Code
########################
# ../codes/model_tfmr.py:1
            torch.tril(torch.ones(1, 1, max_positions, max_positions))    # 下三角矩阵作为mask

# ../codes/model_tfmr.py:2
        attn_weights = torch.matmul(query, key.transpose(-1, -2))
        causal_mask = self.bias.data[..., :attn_weights.shape[-2], :attn_weights.shape[-2]].to(torch.bool)
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, value)
        return attn_output, attn_weights

# ../codes/model_tfmr.py:3
        batch_size, sequence_length, hidden_size = tensor.size()
        tensor = tensor.view(batch_size, sequence_length, num_heads, attn_head_size)
        tensor = tensor.permute(0, 2, 1, 3).contiguous()
        return tensor

# ../codes/model_tfmr.py:4
        tensor = tensor.permute(0, 2, 1, 3).contiguous()
        batch_size, sequence_length, num_heads, attn_head_size = tensor.size()
        tensor = tensor.view(batch_size, sequence_length, -1)
        return tensor

# ../codes/model_tfmr.py:5
        # HINT: You can refer to Page 39 in lecture 8 for more details
        hidden_states = residual + attn_output
        residual = hidden_states
        hidden_states = residual + self.mlp(self.ln_2(hidden_states))

# ../codes/model_tfmr.py:6
        # self.wpe: 1024*768
        position_ids = torch.arange(past_length, input_shape[-1] + past_length, device=device)
        position_embeds = self.wpe(position_ids.view(-1, input_shape[-1]))

# ../codes/model_tfmr.py:7
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            shift_mask = (shift_labels != PAD_ID).float()
            shift_mask = torch.cat((torch.ones(shift_labels.shape[0], 1).to(labels.device), shift_mask[:, :-1]), dim=1)
            loss = ce_loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)) * shift_mask.view(-1)
            loss = loss.sum() / shift_mask.sum()

# ../codes/model_tfmr.py:8
                        # logits: batch_size * vocab_size
                        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                        sorted_indices_to_remove = cumulative_probs > top_p
                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                        sorted_indices_to_remove[..., 0] = 0

                        logits = logits.view(-1)
                        sorted_indices = sorted_indices + torch.arange(sorted_indices.shape[0], dtype=torch.long, device=device)[:, None] * sorted_indices.shape[1]
                        remove_indexes = torch.masked_select(sorted_indices, sorted_indices_to_remove)
                        logits = logits.scatter(0, remove_indexes, float('-inf'))
                        logits = logits.reshape(sorted_indices.shape[0], -1)

# ../codes/main.py:1
            tgt_ids = input_ids[..., 1:].contiguous()
            input_ids = input_ids[..., :-1].contiguous()
            loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))
            loss_mask = torch.cat((torch.ones(tgt_ids.shape[0], 1).to(device), (tgt_ids != PAD_ID).float()[:, :-1]), dim=1)
            loss = (loss * loss_mask.view(-1)).reshape((ed-st), -1).sum(-1) / loss_mask.sum(-1)


########################
# References
########################

########################
# Other Modifications
########################
# _codes/model_tfmr.py -> ../codes/model_tfmr.py
# 319 +
# _codes/main.py -> ../codes/main.py
# 14 + from torch.utils.tensorboard import SummaryWriter
# 19 + writer = SummaryWriter('log')
# 39 - parser.add_argument("--pretrain_dir", type=str, default="None",
# 39 ?                                                         -    -
# 41 + parser.add_argument("--pretrain_dir", type=str, default=None,
# 241 +             writer.add_scalars(f'train_loss', {f'{args.name}': train_loss}, epoch)
# 242 +             writer.add_scalars(f'val_loss', {f'{args.name}': val_loss}, epoch)
# 243 +             writer.add_scalars(f'val_ppl', {f'{args.name}': val_ppl}, epoch)
# 255 -                 print("Validation perplexity: {:.3f}, becomes larger. Stop training.".format(val_ppl))
# 260 +                 # print("Validation perplexity: {:.3f}, becomes larger. Stop training.".format(val_ppl))
# 260 ?                ++
# 256 -                 break
# 261 +                 # break
# 261 ?                 ++
# 262 +                 print("Validation perplexity: {:.3f}, becomes larger. Before: {:.3f}".format(val_ppl, best_val_ppl))
# 266 -         with open(f"output_{args.decode_strategy}.txt", "w") as fout:
# 272 +         with open(f"output_{args.test}_{args.decode_strategy}_{args.temperature}.txt", "w") as fout:
# 272 ?                                  ++++++++++++               +++++++++++++++++++
# 269 -                 print(k, out)
# 275 +                 # print(k, out)
# 275 ?                 ++

